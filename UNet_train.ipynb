{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8177e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): \\ WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\n",
      "done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Solving environment: - ^C\n",
      "unsuccessful attempt using repodata from current_repodata.json, retrying with next repodata source.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137ae4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, Model\n",
    "\n",
    "file_path = \"/mnt/scratch/yuankeji/ibex-zonation/code/tryByOurselves/SSL/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a9c541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x147a7126ecf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread(file_path + \"image/restored_contours.png\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (3,3),0)\n",
    "ret, binary = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "plt.imshow(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864a62af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading original NRRD file: ./PORTAL_CENTRAL_FULL_SECTION_20250131.nrrd\n",
      "Loading global mask: ./image/restored_contours.png\n",
      "Creating patches...\n",
      "Created 95 patches\n",
      "Epoch 1/50, Train Loss: 0.4600, Val Loss: 0.6571, Dice Score: 0.0000\n",
      "Model saved with Dice score: 0.0000\n",
      "Epoch 2/50, Train Loss: 0.3656, Val Loss: 0.6058, Dice Score: 0.0000\n",
      "Epoch 3/50, Train Loss: 0.3211, Val Loss: 0.5445, Dice Score: 0.0000\n",
      "Epoch 4/50, Train Loss: 0.2878, Val Loss: 0.4792, Dice Score: 0.0000\n",
      "Epoch 5/50, Train Loss: 0.2943, Val Loss: 0.4369, Dice Score: 0.0000\n",
      "Epoch 6/50, Train Loss: 0.2696, Val Loss: 0.4119, Dice Score: 0.0000\n",
      "Epoch 7/50, Train Loss: 0.2503, Val Loss: 0.3908, Dice Score: 0.0000\n",
      "Epoch 8/50, Train Loss: 0.2412, Val Loss: 0.3817, Dice Score: 0.0000\n",
      "Epoch 9/50, Train Loss: 0.2324, Val Loss: 0.3787, Dice Score: 0.0000\n",
      "Epoch 10/50, Train Loss: 0.2190, Val Loss: 0.3698, Dice Score: 0.0000\n",
      "Epoch 11/50, Train Loss: 0.2196, Val Loss: 0.3168, Dice Score: 0.3411\n",
      "Model saved with Dice score: 0.3411\n",
      "Epoch 12/50, Train Loss: 0.2132, Val Loss: 0.3020, Dice Score: 0.5345\n",
      "Model saved with Dice score: 0.5345\n",
      "Epoch 13/50, Train Loss: 0.2050, Val Loss: 0.2484, Dice Score: 0.7864\n",
      "Model saved with Dice score: 0.7864\n",
      "Epoch 14/50, Train Loss: 0.2098, Val Loss: 0.2707, Dice Score: 0.7502\n",
      "Epoch 15/50, Train Loss: 0.2058, Val Loss: 0.2312, Dice Score: 0.8452\n",
      "Model saved with Dice score: 0.8452\n",
      "Epoch 16/50, Train Loss: 0.1976, Val Loss: 0.2006, Dice Score: 0.9129\n",
      "Model saved with Dice score: 0.9129\n",
      "Epoch 17/50, Train Loss: 0.1898, Val Loss: 0.1987, Dice Score: 0.9239\n",
      "Model saved with Dice score: 0.9239\n",
      "Epoch 18/50, Train Loss: 0.1829, Val Loss: 0.1946, Dice Score: 0.9294\n",
      "Model saved with Dice score: 0.9294\n",
      "Epoch 19/50, Train Loss: 0.1968, Val Loss: 0.1947, Dice Score: 0.9203\n",
      "Epoch 20/50, Train Loss: 0.1836, Val Loss: 0.1925, Dice Score: 0.9075\n",
      "Epoch 21/50, Train Loss: 0.1826, Val Loss: 0.1792, Dice Score: 0.9237\n",
      "Epoch 22/50, Train Loss: 0.1726, Val Loss: 0.1725, Dice Score: 0.9437\n",
      "Model saved with Dice score: 0.9437\n",
      "Epoch 23/50, Train Loss: 0.1657, Val Loss: 0.1788, Dice Score: 0.9217\n",
      "Epoch 24/50, Train Loss: 0.1639, Val Loss: 0.1634, Dice Score: 0.9535\n",
      "Model saved with Dice score: 0.9535\n",
      "Epoch 25/50, Train Loss: 0.1576, Val Loss: 0.1613, Dice Score: 0.9529\n",
      "Epoch 26/50, Train Loss: 0.1541, Val Loss: 0.1573, Dice Score: 0.9600\n",
      "Model saved with Dice score: 0.9600\n",
      "Epoch 27/50, Train Loss: 0.1532, Val Loss: 0.1556, Dice Score: 0.9522\n",
      "Epoch 28/50, Train Loss: 0.1523, Val Loss: 0.1504, Dice Score: 0.9627\n",
      "Model saved with Dice score: 0.9627\n",
      "Epoch 29/50, Train Loss: 0.1593, Val Loss: 0.1532, Dice Score: 0.9498\n",
      "Epoch 30/50, Train Loss: 0.1547, Val Loss: 0.1508, Dice Score: 0.9455\n",
      "Epoch 31/50, Train Loss: 0.1504, Val Loss: 0.1529, Dice Score: 0.9385\n",
      "Epoch 32/50, Train Loss: 0.1433, Val Loss: 0.1424, Dice Score: 0.9582\n",
      "Epoch 33/50, Train Loss: 0.1432, Val Loss: 0.1504, Dice Score: 0.9481\n",
      "Epoch 34/50, Train Loss: 0.1384, Val Loss: 0.1495, Dice Score: 0.9489\n",
      "Epoch 35/50, Train Loss: 0.1396, Val Loss: 0.1536, Dice Score: 0.9458\n",
      "Epoch 36/50, Train Loss: 0.1381, Val Loss: 0.1459, Dice Score: 0.9496\n",
      "Epoch 37/50, Train Loss: 0.1354, Val Loss: 0.1402, Dice Score: 0.9541\n",
      "Epoch 38/50, Train Loss: 0.1336, Val Loss: 0.1322, Dice Score: 0.9624\n",
      "Epoch 39/50, Train Loss: 0.1345, Val Loss: 0.1410, Dice Score: 0.9445\n",
      "Epoch 40/50, Train Loss: 0.1305, Val Loss: 0.1411, Dice Score: 0.9479\n",
      "Epoch 41/50, Train Loss: 0.1277, Val Loss: 0.1311, Dice Score: 0.9606\n",
      "Epoch 42/50, Train Loss: 0.1286, Val Loss: 0.1292, Dice Score: 0.9657\n",
      "Model saved with Dice score: 0.9657\n",
      "Epoch 43/50, Train Loss: 0.1267, Val Loss: 0.1307, Dice Score: 0.9597\n",
      "Epoch 44/50, Train Loss: 0.1262, Val Loss: 0.1286, Dice Score: 0.9595\n",
      "Epoch 45/50, Train Loss: 0.1281, Val Loss: 0.1344, Dice Score: 0.9489\n",
      "Epoch 46/50, Train Loss: 0.1259, Val Loss: 0.1279, Dice Score: 0.9553\n",
      "Epoch 47/50, Train Loss: 0.1233, Val Loss: 0.1226, Dice Score: 0.9629\n",
      "Epoch 48/50, Train Loss: 0.1204, Val Loss: 0.1233, Dice Score: 0.9567\n",
      "Epoch 49/50, Train Loss: 0.1186, Val Loss: 0.1227, Dice Score: 0.9622\n",
      "Epoch 50/50, Train Loss: 0.1216, Val Loss: 0.1183, Dice Score: 0.9679\n",
      "Model saved with Dice score: 0.9679\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/local/52102077/ipykernel_2709114/2553290298.py:418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to 'prediction_result.png'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "try:\n",
    "    import nrrd\n",
    "except ImportError:\n",
    "    try:\n",
    "        import pynrrd as nrrd\n",
    "        print(\"Using pynrrd instead of nrrd\")\n",
    "    except ImportError:\n",
    "        print(\"Error: Cannot import nrrd or pynrrd library\")\n",
    "        exit(1)\n",
    "\n",
    "# 设置随机种子以保证可重复性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 解除PIL的图像大小限制\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# 定义U-Net模型\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # 输入是BCHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                                   diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 定义数据集类\n",
    "class VesselSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_patches, mask_patches, transform=None):\n",
    "        self.image_patches = image_patches\n",
    "        self.mask_patches = mask_patches\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image_patches[idx]\n",
    "        mask = self.mask_patches[idx]\n",
    "        \n",
    "        # 确保数据类型正确\n",
    "        image = image.astype(np.float32)\n",
    "        mask = mask.astype(np.float32)\n",
    "        \n",
    "        # 转换为张量\n",
    "        image_tensor = torch.from_numpy(image).unsqueeze(0)  # 添加通道维度 (C,H,W)\n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0)  # 添加通道维度 (C,H,W)\n",
    "        \n",
    "        # 应用变换（如果有）\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "            mask_tensor = self.transform(mask_tensor)\n",
    "        \n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "# 创建数据预处理和增强函数\n",
    "def create_patches(image, mask, patch_size=256, stride=128):\n",
    "    \"\"\"从大图像和掩码中创建小块数据\"\"\"\n",
    "    image_patches = []\n",
    "    mask_patches = []\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    for y in range(0, h - patch_size + 1, stride):\n",
    "        for x in range(0, w - patch_size + 1, stride):\n",
    "            # 提取图像小块\n",
    "            image_patch = image[y:y + patch_size, x:x + patch_size]\n",
    "            \n",
    "            # 提取对应的掩码小块\n",
    "            mask_patch = mask[y:y + patch_size, x:x + patch_size]\n",
    "            \n",
    "            # 只保留含有标注的小块（减少负样本的比例）\n",
    "            if np.sum(mask_patch) > 100:  # 如果小块中有足够多的标注像素\n",
    "                image_patches.append(image_patch)\n",
    "                mask_patches.append(mask_patch)\n",
    "    \n",
    "    return image_patches, mask_patches\n",
    "\n",
    "# 数据加载和预处理函数\n",
    "def prepare_data(nrrd_filename, global_mask_path, patch_size=256, stride=128, test_size=0.2):\n",
    "    # 加载原始图像\n",
    "    print(f\"Loading original NRRD file: {nrrd_filename}\")\n",
    "    image, header = nrrd.read(nrrd_filename)\n",
    "    image = image.astype(np.float32, copy=True)\n",
    "    \n",
    "    # 加载全局掩码\n",
    "    print(f\"Loading global mask: {global_mask_path}\")\n",
    "    global_mask = np.array(Image.open(global_mask_path))\n",
    "    if len(global_mask.shape) > 2:\n",
    "        global_mask = global_mask[:, :, 0]  # 只取一个通道\n",
    "    global_mask = (global_mask > 0).astype(np.float32)  # 二值化\n",
    "    \n",
    "    # 确保图像和掩码尺寸匹配\n",
    "    if image.shape[:2] != global_mask.shape:\n",
    "        print(f\"Warning: Image shape {image.shape[:2]} does not match mask shape {global_mask.shape}\")\n",
    "        # 需要调整大小的逻辑\n",
    "    \n",
    "    # 如果图像是多通道的，转换为单通道\n",
    "    if len(image.shape) == 3:\n",
    "        # 使用均值简化为单通道\n",
    "        image = np.mean(image, axis=2)\n",
    "    \n",
    "    # 归一化图像\n",
    "    image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "    \n",
    "    # 创建小块数据\n",
    "    print(\"Creating patches...\")\n",
    "    image_patches, mask_patches = create_patches(image, global_mask, patch_size, stride)\n",
    "    print(f\"Created {len(image_patches)} patches\")\n",
    "    \n",
    "    # 划分训练和验证集\n",
    "    patches_train, patches_val, masks_train, masks_val = train_test_split(\n",
    "        image_patches, mask_patches, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    return patches_train, patches_val, masks_train, masks_val\n",
    "\n",
    "# 评估函数\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1.0\n",
    "    y_true_flat = y_true.view(-1)\n",
    "    y_pred_flat = y_pred.view(-1)\n",
    "    intersection = (y_true_flat * y_pred_flat).sum()\n",
    "    return (2. * intersection + smooth) / (y_true_flat.sum() + y_pred_flat.sum() + smooth)\n",
    "\n",
    "# 训练函数\n",
    "def train_model(model, train_loader, val_loader, device, epochs=100, learning_rate=1e-4):\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 用于记录训练过程\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_dice_scores = []\n",
    "    best_dice = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # 计算平均训练损失\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        dice_score = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                dice_score += dice_coefficient(masks, preds).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        avg_dice = dice_score / len(val_loader)\n",
    "        val_dice_scores.append(avg_dice)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Dice Score: {avg_dice:.4f}')\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if avg_dice > best_dice:\n",
    "            best_dice = avg_dice\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'Model saved with Dice score: {best_dice:.4f}')\n",
    "    \n",
    "    # 绘制训练过程\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_dice_scores, label='Dice Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Dice Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return model, train_losses, val_losses, val_dice_scores\n",
    "\n",
    "# 预测函数\n",
    "def predict(model, image_path, device, patch_size=256, overlap=0.5):\n",
    "    \"\"\"对新图像进行分割预测\"\"\"\n",
    "    # 加载图像\n",
    "    try:\n",
    "        if image_path.endswith('.nrrd'):\n",
    "            image, header = nrrd.read(image_path)\n",
    "            if len(image.shape) == 3:\n",
    "                image = np.mean(image, axis=2)  # 转换为单通道\n",
    "        else:\n",
    "            image = np.array(Image.open(image_path).convert('L'))  # 转为灰度图\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 归一化\n",
    "    image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "    \n",
    "    # 创建结果掩码\n",
    "    result_mask = np.zeros(image.shape, dtype=np.float32)\n",
    "    count_mask = np.zeros(image.shape, dtype=np.float32)\n",
    "    \n",
    "    stride = int(patch_size * (1 - overlap))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for y in range(0, image.shape[0] - patch_size + 1, stride):\n",
    "            for x in range(0, image.shape[1] - patch_size + 1, stride):\n",
    "                # 提取小块\n",
    "                patch = image[y:y + patch_size, x:x + patch_size]\n",
    "                \n",
    "                # 转换为张量\n",
    "                patch_tensor = torch.from_numpy(patch).float().unsqueeze(0).unsqueeze(0)\n",
    "                patch_tensor = patch_tensor.to(device)\n",
    "                \n",
    "                # 预测\n",
    "                output = model(patch_tensor)\n",
    "                pred = torch.sigmoid(output).cpu().numpy().squeeze()\n",
    "                \n",
    "                # 将预测结果放回对应位置\n",
    "                result_mask[y:y + patch_size, x:x + patch_size] += pred\n",
    "                count_mask[y:y + patch_size, x:x + patch_size] += 1\n",
    "    \n",
    "    # 取平均值\n",
    "    result_mask = np.divide(result_mask, count_mask, out=np.zeros_like(result_mask), where=count_mask != 0)\n",
    "    \n",
    "    # 二值化结果\n",
    "    binary_mask = (result_mask > 0.5).astype(np.uint8) * 255\n",
    "    \n",
    "    return binary_mask\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 参数设置\n",
    "    NRRD_FILENAME = \"./PORTAL_CENTRAL_FULL_SECTION_20250131.nrrd\"\n",
    "    GLOBAL_MASK_PATH = \"./image/restored_contours.png\"\n",
    "    PATCH_SIZE = 256\n",
    "    STRIDE = 128\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    # 检查CUDA是否可用\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 准备数据\n",
    "    patches_train, patches_val, masks_train, masks_val = prepare_data(\n",
    "        NRRD_FILENAME, \n",
    "        GLOBAL_MASK_PATH, \n",
    "        patch_size=PATCH_SIZE,\n",
    "        stride=STRIDE\n",
    "    )\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    train_dataset = VesselSegmentationDataset(patches_train, masks_train)\n",
    "    val_dataset = VesselSegmentationDataset(patches_val, masks_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = UNet(n_channels=1, n_classes=1, bilinear=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 训练模型\n",
    "    trained_model, train_losses, val_losses, val_dice_scores = train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        device, \n",
    "        epochs=EPOCHS, \n",
    "        learning_rate=LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "#     示例：预测新图像\n",
    "    new_image_path = \"./PORTAL_CENTRAL_FULL_SECTION_20250131.nrrd\"\n",
    "    result = predict(model, new_image_path, device, PATCH_SIZE, overlap=0.5)\n",
    "    if result is not None:\n",
    "        # 保存结果\n",
    "        Image.fromarray(result).save('prediction_result.png')\n",
    "        print(\"Prediction saved to 'prediction_result.png'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ef6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
